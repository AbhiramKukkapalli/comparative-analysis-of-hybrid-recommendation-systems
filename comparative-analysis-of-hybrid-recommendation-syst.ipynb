{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":960949,"sourceType":"datasetVersion","datasetId":517594}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport nltk\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\nimport os\n!pip install indic-nlp-library\nfrom indicnlp.tokenize import sentence_tokenize, indic_tokenize\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-07T15:23:19.021872Z","iopub.execute_input":"2023-12-07T15:23:19.022433Z","iopub.status.idle":"2023-12-07T15:23:52.694548Z","shell.execute_reply.started":"2023-12-07T15:23:19.022364Z","shell.execute_reply":"2023-12-07T15:23:52.693208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_path = \"../input/telugu-nlp/telugu_news/train_telugu_news.csv\"\ntelugu_news_df = pd.read_csv(train_path)\ntelugu_news_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:52.696473Z","iopub.execute_input":"2023-12-07T15:23:52.697036Z","iopub.status.idle":"2023-12-07T15:23:54.639606Z","shell.execute_reply.started":"2023-12-07T15:23:52.697003Z","shell.execute_reply":"2023-12-07T15:23:54.638440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text pre-processing","metadata":{}},{"cell_type":"code","source":"# Remove rows where 'heading' is null\ntelugu_news_df[telugu_news_df[\"heading\"].isna() == True]","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:54.641067Z","iopub.execute_input":"2023-12-07T15:23:54.641465Z","iopub.status.idle":"2023-12-07T15:23:54.660892Z","shell.execute_reply.started":"2023-12-07T15:23:54.641420Z","shell.execute_reply":"2023-12-07T15:23:54.659802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_dic = {}\n\nc = 0\nfor un in telugu_news_df[\"topic\"].unique():\n    if un not in topic_dic:\n        topic_dic[un] = c\n        c += 1\n        \ntopic_dic","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:54.664161Z","iopub.execute_input":"2023-12-07T15:23:54.664767Z","iopub.status.idle":"2023-12-07T15:23:54.692961Z","shell.execute_reply.started":"2023-12-07T15:23:54.664733Z","shell.execute_reply":"2023-12-07T15:23:54.691485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inv_topic_dict = {v: k for k, v in topic_dic.items()}","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:54.694505Z","iopub.execute_input":"2023-12-07T15:23:54.694911Z","iopub.status.idle":"2023-12-07T15:23:54.701284Z","shell.execute_reply.started":"2023-12-07T15:23:54.694875Z","shell.execute_reply":"2023-12-07T15:23:54.700171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def func_topic(s):\n    return topic_dic[s]\n\ntelugu_news_df[\"topic\"] = telugu_news_df[\"topic\"].apply(func_topic)\ndate_df = telugu_news_df[\"date\"]\ntelugu_news_df[\"body_processed\"] = telugu_news_df[\"body\"].str.replace('\\u200c', '')\ntelugu_news_df[\"body_processed\"] = telugu_news_df[\"body_processed\"].str.replace('\\n', '')\ntelugu_news_df[\"body_processed\"] = telugu_news_df[\"body_processed\"].str.replace('\\t', '')\ntelugu_news_df[\"body_processed\"] = telugu_news_df[\"body_processed\"].str.replace('\\xa0', '')","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:54.703216Z","iopub.execute_input":"2023-12-07T15:23:54.704095Z","iopub.status.idle":"2023-12-07T15:23:55.000172Z","shell.execute_reply.started":"2023-12-07T15:23:54.704054Z","shell.execute_reply":"2023-12-07T15:23:54.998809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_count(df):\n    \n    unvals = list(df.unique())\n    op = [0]*len(unvals)\n    \n    i = 0\n    for un in unvals:\n        op[i] = df[df == un].shape[0]\n        i += 1\n        \n    return [list(unvals),op]\n\ncont = get_count(telugu_news_df[\"topic\"])\nclables = cont[0]\n\nlabels = []\nfor c in clables:\n    labels.append(inv_topic_dict[c])\n\nplt.figure(figsize=(10,8))\nplt.title(\"Distribution of the telugu news toics\", fontsize = 14.5)\nplt.style.use('seaborn-colorblind')\nplt.pie(np.array(cont[1]), labels=labels, autopct='%1.2f%%', shadow=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:55.001529Z","iopub.execute_input":"2023-12-07T15:23:55.001869Z","iopub.status.idle":"2023-12-07T15:23:55.366686Z","shell.execute_reply.started":"2023-12-07T15:23:55.001841Z","shell.execute_reply":"2023-12-07T15:23:55.365645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PUNCT = string.punctuation\n\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCT))\ntelugu_news_df[\"body_processed\"] = telugu_news_df[\"body_processed\"].apply(lambda text: remove_punctuation(text))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:55.368049Z","iopub.execute_input":"2023-12-07T15:23:55.369200Z","iopub.status.idle":"2023-12-07T15:23:57.869399Z","shell.execute_reply.started":"2023-12-07T15:23:55.369158Z","shell.execute_reply":"2023-12-07T15:23:57.868428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = \"../input/telugu-nlp/telugu_news/test_telugu_news.csv\"\n\ntest_news_df = pd.read_csv(test_path)\n\ntest_news_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:57.870848Z","iopub.execute_input":"2023-12-07T15:23:57.871156Z","iopub.status.idle":"2023-12-07T15:23:58.342228Z","shell.execute_reply.started":"2023-12-07T15:23:57.871130Z","shell.execute_reply":"2023-12-07T15:23:58.341273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = test_news_df[\"topic\"].apply(func_topic)\ntest_news_df[\"body_processed\"] = test_news_df[\"body\"].str.replace('\\u200c', '')\ntest_news_df[\"body_processed\"] = test_news_df[\"body_processed\"].str.replace('\\n', '')\ntest_news_df[\"body_processed\"] = test_news_df[\"body_processed\"].str.replace('\\t', '')\ntest_news_df[\"body_processed\"] = test_news_df[\"body_processed\"].str.replace('\\xa0', '')\n\ntest_news_df[\"body_processed\"] = test_news_df[\"body_processed\"].apply(lambda text: remove_punctuation(text))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:58.346502Z","iopub.execute_input":"2023-12-07T15:23:58.346820Z","iopub.status.idle":"2023-12-07T15:23:59.037978Z","shell.execute_reply.started":"2023-12-07T15:23:58.346792Z","shell.execute_reply":"2023-12-07T15:23:59.036901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Count Vectorizer to get the data into sk-learn's format\nUsing Count Vectorizer to get the feature vectors and eliminate the stopwords (based on term and inverse document freqency and selecting the top k words in the vacabulary for model development purpose)","metadata":{}},{"cell_type":"code","source":"categories = [i for i in range(5)]\n\ntest_text = []\nfor t in test_news_df[\"body_processed\"]:\n  test_text.append(t)\n\nx_test = test_text\n\nprint(len(x_test) , len(y_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:59.039621Z","iopub.execute_input":"2023-12-07T15:23:59.040003Z","iopub.status.idle":"2023-12-07T15:23:59.048401Z","shell.execute_reply.started":"2023-12-07T15:23:59.039968Z","shell.execute_reply":"2023-12-07T15:23:59.047252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\ncategories = [i for i in range(5)]\n\ntext_topic = []\nfor i in range(5):\n  curr_text = \"\"\n\n  for text in telugu_news_df[telugu_news_df[\"topic\"] == i][\"body_processed\"]:\n    curr_text += text\n    curr_text += \" \"\n    \n  text_topic.append(curr_text)\n\nlen(text_topic)\n\nfor i in range(5):\n  print(len(text_topic[i]))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:59.049959Z","iopub.execute_input":"2023-12-07T15:23:59.050304Z","iopub.status.idle":"2023-12-07T15:23:59.256866Z","shell.execute_reply.started":"2023-12-07T15:23:59.050274Z","shell.execute_reply":"2023-12-07T15:23:59.255643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from indicnlp.tokenize import indic_tokenize  \n\n\ndef get_all_vocab(tot_text):\n  dic = {}\n  for t in indic_tokenize.trivial_tokenize(tot_text): \n    if t not in dic:\n      dic[t] = 1\n    else:\n      dic[t] += 1\n\n  return dic \n  \ntot_text = \"\"\nfor i in range(5):\n  tot_text += text_topic[i]","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:59.258348Z","iopub.execute_input":"2023-12-07T15:23:59.258822Z","iopub.status.idle":"2023-12-07T15:23:59.313151Z","shell.execute_reply.started":"2023-12-07T15:23:59.258783Z","shell.execute_reply":"2023-12-07T15:23:59.311859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tot_vocab = get_all_vocab(tot_text)\ntot_vocab = {k: v for k, v in sorted(tot_vocab.items(), key=lambda item: item[1], reverse = True)}\n\nprint(len(tot_vocab))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:23:59.314904Z","iopub.execute_input":"2023-12-07T15:23:59.315400Z","iopub.status.idle":"2023-12-07T15:24:07.259549Z","shell.execute_reply.started":"2023-12-07T15:23:59.315336Z","shell.execute_reply":"2023-12-07T15:24:07.258312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = text_topic\ny_train = categories","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:07.261047Z","iopub.execute_input":"2023-12-07T15:24:07.261407Z","iopub.status.idle":"2023-12-07T15:24:07.266841Z","shell.execute_reply.started":"2023-12-07T15:24:07.261353Z","shell.execute_reply":"2023-12-07T15:24:07.265667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import regex \nfrom indicnlp.tokenize import indic_tokenize\n\n# Using custom analyser for the count vectorizer (as telugu is an indeic language)\ndef custom_analyzer(text):\n    words = regex.findall(r'\\w{1,}', text) #extract words of at least 2 letters\n    for w in words:\n        yield w","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:07.268318Z","iopub.execute_input":"2023-12-07T15:24:07.268700Z","iopub.status.idle":"2023-12-07T15:24:07.282858Z","shell.execute_reply.started":"2023-12-07T15:24:07.268671Z","shell.execute_reply":"2023-12-07T15:24:07.281553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We are using both uni-grams and bi-grams here to get the feature vectors\n\n# We selected only the top 100000 words from the corpus to represent our data\n\ncount_vec = CountVectorizer(max_df = 0.75,min_df=0.1, lowercase = False , analyzer = custom_analyzer, max_features=100000, ngram_range=(1,2))\n\nx_train_features = count_vec.fit_transform(x_train)\n\nx_train_features.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:07.284474Z","iopub.execute_input":"2023-12-07T15:24:07.284817Z","iopub.status.idle":"2023-12-07T15:24:13.142577Z","shell.execute_reply.started":"2023-12-07T15:24:07.284788Z","shell.execute_reply":"2023-12-07T15:24:13.141431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stop words identified from the corpus by using the term-frequencies and the inverse document frequencies\nlen(count_vec.stop_words_)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:13.144003Z","iopub.execute_input":"2023-12-07T15:24:13.144679Z","iopub.status.idle":"2023-12-07T15:24:13.152153Z","shell.execute_reply.started":"2023-12-07T15:24:13.144636Z","shell.execute_reply":"2023-12-07T15:24:13.150979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the testing data's features\nx_test_features = count_vec.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:13.153705Z","iopub.execute_input":"2023-12-07T15:24:13.154052Z","iopub.status.idle":"2023-12-07T15:24:14.386391Z","shell.execute_reply.started":"2023-12-07T15:24:13.153995Z","shell.execute_reply":"2023-12-07T15:24:14.385362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Multinomial Naive Baye's classifier from sk-learn to classify the given telugu texts in the test dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB()\nclf.fit(x_train_features, y_train)\nMultinomialNB()\n\nprint(\"Test score :- \", clf.score(x_test_features, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:14.387870Z","iopub.execute_input":"2023-12-07T15:24:14.388300Z","iopub.status.idle":"2023-12-07T15:24:14.418670Z","shell.execute_reply.started":"2023-12-07T15:24:14.388261Z","shell.execute_reply":"2023-12-07T15:24:14.417443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ny_pred_test = clf.predict(x_test_features)\ntarget_names = list(inv_topic_dict.values())\n\nprint(classification_report(y_test, y_pred_test, target_names=target_names))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:14.420168Z","iopub.execute_input":"2023-12-07T15:24:14.421037Z","iopub.status.idle":"2023-12-07T15:24:14.450397Z","shell.execute_reply.started":"2023-12-07T15:24:14.420994Z","shell.execute_reply":"2023-12-07T15:24:14.449305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report,accuracy_score,average_precision_score,f1_score\nconfusion_mat = confusion_matrix(y_test, y_pred_test)\n\nplt.figure(figsize=(10,4))\nplt.title(\"Confusion matrix for test data\")\nplt.xlabel(\"Predicted class\")\nplt.ylabel(\"True class\")\n\nsns.heatmap(confusion_mat, annot=True, cmap=\"Greens\",  xticklabels = target_names,\n           yticklabels=target_names)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:14.451806Z","iopub.execute_input":"2023-12-07T15:24:14.452158Z","iopub.status.idle":"2023-12-07T15:24:14.874841Z","shell.execute_reply.started":"2023-12-07T15:24:14.452126Z","shell.execute_reply":"2023-12-07T15:24:14.873701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for predicting a sample text with the trained model\n\ndef predict_text_sample(test_text, inv_topic_dict, clf, count_vec):\n  test_sample = [test_text]\n  x_test_sample_fetaures = count_vec.transform(test_sample)\n  y_pred_test_sample = clf.predict(x_test_sample_fetaures)\n\n  return inv_topic_dict[y_pred_test_sample[0]]","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:14.876599Z","iopub.execute_input":"2023-12-07T15:24:14.877286Z","iopub.status.idle":"2023-12-07T15:24:14.883788Z","shell.execute_reply.started":"2023-12-07T15:24:14.877246Z","shell.execute_reply":"2023-12-07T15:24:14.882715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This text is about some movie news from a telugu blogsite\ntest_text =\"ఇండియన్ స్క్రీన్ మీద పోటీ పడటం అయిపోయింది అందుకే ఇప్పుడు మన సినిమాలు ఫారిన్ రిలీజ్ లో పోటీ పడుతున్నాయి. ఇండియన్ సినిమాలు ముఖ్యంగా సౌత్ సినిమాలు రిలీజ్ అవుతున్నాయి అంటే వరల్డ్ వైడ్ మార్కెట్ ఓపెన్ అవుతుంది. తెలుగు తమిళ హిందీ భాషల సినిమాలు సబ్ టైటిల్స్ తో విధేశాల్లో కూడా రిలీజ్ అవుతున్నాయి\"\n\nprint(\"Prediced class is \" , predict_text_sample(test_text, inv_topic_dict, clf, count_vec))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:14.885059Z","iopub.execute_input":"2023-12-07T15:24:14.885627Z","iopub.status.idle":"2023-12-07T15:24:14.898596Z","shell.execute_reply.started":"2023-12-07T15:24:14.885596Z","shell.execute_reply":"2023-12-07T15:24:14.897445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This sample text is about some political news from a telugu news website (Eenadu)\n\ntest_text = \"హైదరాబాద్: తెలంగాణలో సంచలనం సృష్టించిన ‘ఎమ్మెల్యేలకు ఎర’ కేసులో హైకోర్టు కీలక తీర్పు వెల్లడించింది. ఈ కేసులో ముగ్గురు నిందితుల రిమాండ్‌కు ఉన్నత న్యాయస్థానం అనుమతించింది. నిందితులు వెంటనే సైబరాబాద్ కమిషనర్‌ స్టీఫెన్‌ రవీంద్ర ఎదుట లొంగిపోవాలని ఆదేశించింది. ఒకవేళ లొంగిపోకపోతే వారిని అరెస్టు చేసి ఏసీబీ కోర్టులో హాజరుపర్చాలని.. ఆ తర్వాత రిమాండ్‌కు తరలించాలని పోలీసులను ధర్మాసనం ఆదేశించింది.\"\nprint(\"Prediced class is \" , predict_text_sample(test_text, inv_topic_dict, clf, count_vec))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:14.900230Z","iopub.execute_input":"2023-12-07T15:24:14.901097Z","iopub.status.idle":"2023-12-07T15:24:14.913199Z","shell.execute_reply.started":"2023-12-07T15:24:14.901057Z","shell.execute_reply":"2023-12-07T15:24:14.912019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This sample text is about some sports news from a telugu news website (Eenadu)\n\ntest_text = \"జట్టు స్కోరు 3 పరుగుల వద్ద రెండో ఓవర్లోనే బెయిర్ స్టో అవుట్ కాగా.. ఆ తర్వాత అఫ్ఘానిస్తా్న్ బౌలర్ల ఉచ్చులో ఇంగ్లండ్ బ్యాటర్లు విలవిలలాడిపోయారు. \"\nprint(\"Prediced class is \" , predict_text_sample(test_text, inv_topic_dict, clf, count_vec))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:14.915009Z","iopub.execute_input":"2023-12-07T15:24:14.915385Z","iopub.status.idle":"2023-12-07T15:24:14.925064Z","shell.execute_reply.started":"2023-12-07T15:24:14.915339Z","shell.execute_reply":"2023-12-07T15:24:14.923791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Neural Networks to classify the given telugu texts","metadata":{}},{"cell_type":"code","source":"# from tensorflow.keras.preprocessing.text import Tokenizer\n# from tensorflow.keras.preprocessing.sequence import pad_sequences\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Embedding, LSTM, Dense\n# from tensorflow.keras.utils import to_categorical  # Import to_categorical\n\n# # Preprocessing\n# tokenizer = Tokenizer(num_words=5000)  # Adjust 'num_words' as needed\n# tokenizer.fit_on_texts(telugu_news_df['body'])\n# sequences = tokenizer.texts_to_sequences(telugu_news_df['body'])\n# data = pad_sequences(sequences, maxlen=200)  # Adjust 'maxlen' as needed\n# # Check the number of unique categories in 'topic'\n# num_categories = len(telugu_news_df['topic'].unique())\n\n# # Adjust the final Dense layer of your model to match the number of categories\n# model = Sequential()\n# model.add(Embedding(input_dim=5000, output_dim=64, input_length=200))  # Adjust these dimensions as needed\n# model.add(LSTM(64, return_sequences=True))\n# model.add(LSTM(32))\n# model.add(Dense(num_categories, activation='softmax'))  # Ensure this matches the number of categories\n\n# # Compile the model\n# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# # Convert labels to categorical\n# labels = to_categorical(telugu_news_df['topic'], num_classes=num_categories)\n\n# # Train the model\n# model.fit(data, labels, epochs=10, batch_size=32)  # Adjust epochs and batch_size as needed\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:14.928198Z","iopub.execute_input":"2023-12-07T15:24:14.928637Z","iopub.status.idle":"2023-12-07T15:24:14.938287Z","shell.execute_reply.started":"2023-12-07T15:24:14.928600Z","shell.execute_reply":"2023-12-07T15:24:14.937350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def predict_category(text, tokenizer, model, inv_topic_dict):\n#     # Preprocess the text\n#     sequence = tokenizer.texts_to_sequences([text])\n#     padded_sequence = pad_sequences(sequence, maxlen=200)\n\n#     # Predict\n#     prediction = model.predict(padded_sequence)\n#     predicted_category_index = np.argmax(prediction)\n#     predicted_category = inv_topic_dict[predicted_category_index]\n\n#     return predicted_category","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:14.944237Z","iopub.execute_input":"2023-12-07T15:24:14.944597Z","iopub.status.idle":"2023-12-07T15:24:14.952482Z","shell.execute_reply.started":"2023-12-07T15:24:14.944569Z","shell.execute_reply":"2023-12-07T15:24:14.951598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Example\n# sample_text = \"ఆగండ్రా బాబూ రేయ్ ఆగండ్రే.. అంటూ తన ఫ్యాన్స్‌ కేరింతల్ని రెట్టింపు చేస్తూ తన మాస్ మేనరిజంతో స్పీచ్ ఇచ్చారు రవితేజ. ఈ సినిమా ప్రతి బ్లాక్.. అంత అందంగా వచ్చిందంటే.. సినిమాటోగ్రాఫర్ మది.. యాక్షన్ ఎపిసోడ్ చేసిన రామ్ లక్ష్మణ్, పీటర్ హెయిన్స్. ఆ ట్రైన్ ఎపిసోడ్‌ చేసింది పీటర్ హెయిన్స్. ఈ సినిమా రియల్ క్యారెక్టర్ కాబట్టి.. రియల్ ఎమోషన్స్‌తో సినిమా చేశారు. రియల్ యాక్షన్ ఎమోషన్స్ అంత బాగా రావడానికి కారణం ఏంటంటే.. రామ్ లక్ష్మణ్‌లు కూడా.. టైగర్ నాగేశ్వరరావు ఏరియాకి చెందిన చీరాల వారే. ఈ టైగర్ నాగేశ్వరరావు గురించి బాగా తెలిసిన వాళ్లు రామ్ లక్ష్మణ్‌లు.\"\n\n# predicted_category = predict_category(sample_text, tokenizer, model, inv_topic_dict)\n# print(\"Predicted Category:\", predicted_category)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:14.953502Z","iopub.execute_input":"2023-12-07T15:24:14.953806Z","iopub.status.idle":"2023-12-07T15:24:14.964895Z","shell.execute_reply.started":"2023-12-07T15:24:14.953779Z","shell.execute_reply":"2023-12-07T15:24:14.963742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Sample User Data for 100 Users","metadata":{}},{"cell_type":"code","source":"telugu_news_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:14.966116Z","iopub.execute_input":"2023-12-07T15:24:14.966616Z","iopub.status.idle":"2023-12-07T15:24:14.989542Z","shell.execute_reply.started":"2023-12-07T15:24:14.966568Z","shell.execute_reply":"2023-12-07T15:24:14.988393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\n# Assume 100 users \nnum_users = 100\nnum_articles = telugu_news_df.shape[0]\n\n# Assume each user has a preferred topic\nuser_preferences = np.random.randint(0, 5, num_users)\n\n# Assume each user has a unique popularity score for each article\nuser_article_popularity = np.random.rand(num_users, num_articles)\n\n# Simulate user-article interaction matrix\nuser_article_matrix = np.zeros((num_users, num_articles))\n\nfor user_idx in range(num_users):\n    user_topic = user_preferences[user_idx]\n    for article_idx in range(num_articles):\n        if telugu_news_df.loc[article_idx, 'topic'] == user_topic:\n            interaction = np.random.randint(0, 5)\n        else:\n            interaction = np.random.randint(0, 3)\n        user_article_matrix[user_idx, article_idx] = interaction * user_article_popularity[user_idx, article_idx]\n\n# Normalize interactions to scale [0, 4]\nuser_article_matrix = normalize(user_article_matrix, norm='max', axis=1) * 4\n\n# Compute the cosine similarity matrix\nuser_similarity_matrix = cosine_similarity(user_article_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:24:14.990892Z","iopub.execute_input":"2023-12-07T15:24:14.991206Z","iopub.status.idle":"2023-12-07T15:25:00.693401Z","shell.execute_reply.started":"2023-12-07T15:24:14.991179Z","shell.execute_reply":"2023-12-07T15:25:00.691634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Content based Recommendation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:25:00.696062Z","iopub.execute_input":"2023-12-07T15:25:00.696766Z","iopub.status.idle":"2023-12-07T15:25:00.708262Z","shell.execute_reply.started":"2023-12-07T15:25:00.696712Z","shell.execute_reply":"2023-12-07T15:25:00.706317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_ngrams(tokens, n=2):\n    \"\"\"Generate ngrams from tokens.\"\"\"\n    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n\ndef custom_analyzer_with_ngrams(text, n=2):\n    \"\"\"Tokenize text and generate n-grams.\"\"\"\n    words = indic_tokenize.trivial_tokenize(text)\n    ngrams = generate_ngrams(words, n)\n    return words + ngrams  # Return unigrams and bigrams","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:25:00.710434Z","iopub.execute_input":"2023-12-07T15:25:00.711859Z","iopub.status.idle":"2023-12-07T15:25:00.722660Z","shell.execute_reply.started":"2023-12-07T15:25:00.711802Z","shell.execute_reply":"2023-12-07T15:25:00.721123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_recommendations_by_index(article_index, df_content, top_n=10):\n    # Compute tfidf with both unigrams and bigrams\n    vect = TfidfVectorizer(analyzer=custom_analyzer_with_ngrams, ngram_range=(1,2), max_df=0.85, min_df=0.05)\n    count_matrix = vect.fit_transform(df_content.body_processed.values)\n    \n    # Get the tf-idf vector for the specified article\n    article_vector = count_matrix[article_index]\n\n    # Compute the cosine similarity matrix for the specified article\n    cosine_sim = linear_kernel(article_vector, count_matrix).flatten()\n    \n    # Get the similarity scores\n    sim_scores = list(enumerate(cosine_sim))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)  # Sort by similarity\n    sim_scores = sim_scores[1:top_n+1]  # Skip the first one as it is the article itself\n    similar_article_indices = [i[0] for i in sim_scores]\n    scores = [i[1] for i in sim_scores]\n    \n    # Retrieve the similar articles and their scores\n    similar_articles = df_content['body_processed'].iloc[similar_article_indices]\n    return similar_articles, scores\n\n# Example usage\narticle_index = 0  \nrecommended_articles, scores = get_recommendations_by_index(article_index, telugu_news_df, 10)\n\n# Displaying the recommended articles and their similarity scores\nprint(\"Original Article:\", telugu_news_df.iloc[article_index]['body_processed'][:100], \"\\n\")\nfor idx, (article, score) in enumerate(zip(recommended_articles, scores)):\n    print(f\"Score: {score:.4f} - Article: {article[:100]}...\")  # Displaying first 100 chars\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:25:00.725290Z","iopub.execute_input":"2023-12-07T15:25:00.726399Z","iopub.status.idle":"2023-12-07T15:25:16.469230Z","shell.execute_reply.started":"2023-12-07T15:25:00.726331Z","shell.execute_reply":"2023-12-07T15:25:16.468457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Collaborative Filtering Recommendation","metadata":{}},{"cell_type":"markdown","source":"## User-user Filtering","metadata":{}},{"cell_type":"code","source":"def recommend_articles_user_user_with_scores(user_index, df_content, top_n=5):\n    # Find similar users\n    similar_users_scores = user_similarity_matrix[user_index]\n    similar_users_indices = np.argsort(similar_users_scores)[::-1][1:]  # Exclude the user itself\n    \n    # Get the articles liked by similar users\n    recommended_articles_indices = user_article_matrix[similar_users_indices].sum(axis=0)\n    sorted_article_indices = np.argsort(recommended_articles_indices)[::-1]\n\n    # Filter out articles the user has already interacted with and get top N articles\n    recommended_article_indices = [idx for idx in sorted_article_indices if user_article_matrix[user_index, idx] == 0][:top_n]\n    \n    # Get the similarity scores for top N similar users\n    top_similar_users_indices = similar_users_indices[:top_n]\n    top_similar_users_scores = similar_users_scores[top_similar_users_indices]\n\n    # Retrieve the recommended articles\n    recommended_articles = df_content.iloc[recommended_article_indices]\n    return recommended_articles, top_similar_users_scores\n    \n\n# Example usage\nuser_index = 0  # index of the user\nrecommended_articles, user_scores = recommend_articles_user_user_with_scores(user_index, telugu_news_df, 10)\n\n# Displaying the recommended articles and the similarity scores of the users who liked these articles\nfor idx, (article, score) in enumerate(zip(recommended_articles['body'], user_scores)):\n    print(f\"Score: {score:.4f} - Article: {article[:100]}...\")  # Displaying first 100 chars ","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:25:16.470816Z","iopub.execute_input":"2023-12-07T15:25:16.471202Z","iopub.status.idle":"2023-12-07T15:25:16.500309Z","shell.execute_reply.started":"2023-12-07T15:25:16.471173Z","shell.execute_reply":"2023-12-07T15:25:16.499222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Item-Item Filtering","metadata":{}},{"cell_type":"code","source":"# Compute the cosine similarity matrix between articles for Item-Item Collaborative Filtering\narticle_similarity_matrix = cosine_similarity(user_article_matrix.T)\n\n# Function to recommend similar articles for a given article using Item-Item Collaborative Filtering\ndef recommend_similar_articles_with_scores(article_index, df_content, top_n=5):\n    # Compute similarity scores\n    similar_articles_scores = article_similarity_matrix[article_index]\n    \n    # Get top N similar articles, excluding the article itself\n    similar_articles_indices = np.argsort(similar_articles_scores)[::-1][1:top_n+1]\n    top_similar_scores = similar_articles_scores[similar_articles_indices]\n    \n    # Retrieve the recommended articles and their similarity scores\n    recommended_articles = df_content.iloc[similar_articles_indices]\n    return recommended_articles, top_similar_scores\n\n# Example usage\narticle_index = 0  # index of the article \nrecommended_articles, scores = recommend_similar_articles_with_scores(article_index, telugu_news_df, 10)\n\n# Displaying the original article\nprint(\"Original Article:\", telugu_news_df.iloc[article_index]['body'][:100], \"\\n\")\n\n# Displaying the recommended articles and their similarity scores\nfor idx, (article, score) in enumerate(zip(recommended_articles['body'][:100], scores)):\n    print(f\"Score: {score:.4f} - Article: {article[:100]}...\")  # Displaying first 100 chars ","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:25:16.502114Z","iopub.execute_input":"2023-12-07T15:25:16.502551Z","iopub.status.idle":"2023-12-07T15:25:18.650466Z","shell.execute_reply.started":"2023-12-07T15:25:16.502509Z","shell.execute_reply":"2023-12-07T15:25:18.649284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Matrix Factorization using SVD","metadata":{}},{"cell_type":"code","source":"from scipy.linalg import svd\n\n# Performing SVD\nU, sigma, Vt = svd(user_article_matrix)\n\n# Number of latent features \nn_latent_features = 75\n\n# Reduce the matrices U, Sigma, and Vt \nU_reduced = U[:, :n_latent_features]\nsigma_reduced = np.diag(sigma[:n_latent_features])\nVt_reduced = Vt[:n_latent_features, :]\n\n# Reconstruct the user-article interaction matrix\nreconstructed_matrix = np.dot(np.dot(U_reduced, sigma_reduced), Vt_reduced)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:25:18.652397Z","iopub.execute_input":"2023-12-07T15:25:18.652857Z","iopub.status.idle":"2023-12-07T15:25:42.397229Z","shell.execute_reply.started":"2023-12-07T15:25:18.652798Z","shell.execute_reply":"2023-12-07T15:25:42.395701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recommend_articles_svd_with_scores(user_index, df_content, top_n=5):\n    # Predicted user ratings from the reconstructed matrix\n    user_ratings = reconstructed_matrix[user_index]\n\n    # Normalize scores to a 0-1 range for consistency\n    max_rating = np.max(user_ratings)\n    normalized_scores = user_ratings / max_rating if max_rating != 0 else user_ratings\n    \n    # Sort articles by predicted rating, excluding already interacted articles\n    sorted_article_indices = np.argsort(normalized_scores)[::-1]\n    recommended_article_indices = [idx for idx in sorted_article_indices if user_article_matrix[user_index, idx] == 0][:top_n]\n\n    # Retrieve the recommended articles and their normalized scores\n    recommended_articles = df_content.iloc[recommended_article_indices]\n    scores = normalized_scores[recommended_article_indices]\n\n    return recommended_articles, scores\n\n\n# Example usage\nuser_index = 0  # index of the user\nrecommended_articles, scores = recommend_articles_svd_with_scores(user_index, telugu_news_df, 10)\n\n# Displaying the recommended articles and their predicted interest scores\nfor idx, (article, score) in enumerate(zip(recommended_articles['body'], scores)):                                          \n    print(f\"Score: {score:.4f} - Article: {article[:100]}...\")  # Displaying first 100 chars ","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:25:42.399066Z","iopub.execute_input":"2023-12-07T15:25:42.399740Z","iopub.status.idle":"2023-12-07T15:25:42.483761Z","shell.execute_reply.started":"2023-12-07T15:25:42.399701Z","shell.execute_reply":"2023-12-07T15:25:42.482204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hybrid Content-Based Filtering and Matrix Factorization ","metadata":{}},{"cell_type":"code","source":"def weighted_hybrid_recommendations(article_index, user_index, df_content, top_n=10, weight_content_based=0.6, weight_matrix_factorization=0.4):\n    \n    print(f\"User Index: {user_index}\")\n    user_topic_preference = user_preferences[user_index]\n    print(f\"User Topic Preference: {user_topic_preference}\\n\")\n    print(f\"Original Article Index: {article_index}\")\n    print(\"Original Article Details:\")\n    print(\"Title:\", df_content.loc[article_index, 'heading'])\n    print(\"Body:\", df_content.loc[article_index, 'body'][:200] + \"...\\n\")\n    \n    # Get content-based recommendations\n    cb_recommendations, cb_scores = get_recommendations_by_index(article_index, df_content, top_n)\n\n    # Get matrix factorization recommendations\n    mf_recommendations, mf_scores = recommend_articles_svd_with_scores(user_index, df_content, top_n)\n\n    # Combine recommendations and scores with weights\n    combined_scores = {}\n    \n    # Processing content-based scores\n    for idx, score in zip(cb_recommendations.index, cb_scores):\n        body = df_content.loc[idx, 'body_processed']\n        combined_scores[body] = combined_scores.get(body, 0) + score * weight_content_based\n\n    # Processing matrix factorization scores\n    for idx, score in zip(mf_recommendations.index, mf_scores):\n        body = df_content.loc[idx, 'body_processed']\n        combined_scores[body] = combined_scores.get(body, 0) + score * weight_matrix_factorization\n\n    # Sort articles based on combined scores\n    sorted_articles = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n\n    # Get top N recommendations\n    top_articles = [(article, score) for article, score in sorted_articles]\n\n    return top_articles\n\n# Example usage\nuser_index = 0 \narticle_index = 0\ntop_recommendations = weighted_hybrid_recommendations(article_index, user_index, telugu_news_df, top_n=10)\n\n# Displaying the hybrid recommendations\nfor article, score in top_recommendations:\n    print(f\"Score: {score:.4f} - Article: {article[:100]}...\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:31:00.322909Z","iopub.execute_input":"2023-12-07T15:31:00.323349Z","iopub.status.idle":"2023-12-07T15:31:16.221250Z","shell.execute_reply.started":"2023-12-07T15:31:00.323317Z","shell.execute_reply":"2023-12-07T15:31:16.220073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hybrid Content-Based Filtering and User-user filtering","metadata":{}},{"cell_type":"code","source":"def hybrid_recommendations_1(article_index, user_index, df_content, user_similarity_matrix, user_article_matrix, top_n=10, weight_content_based=0.3, weight_user_user=0.7):\n    \n    print(f\"User Index: {user_index}\")\n    user_topic_preference = user_preferences[user_index]\n    print(f\"User Topic Preference: {user_topic_preference}\\n\")\n    print(f\"Original Article Index: {article_index}\")\n    print(\"Original Article Details:\")\n    print(\"Title:\", df_content.loc[article_index, 'heading'])\n    print(\"Body:\", df_content.loc[article_index, 'body'][:200] + \"...\\n\")\n    \n    # Get content-based recommendations\n    cb_recommendations, cb_scores = get_recommendations_by_index(article_index, df_content, top_n)\n\n    # Get user-user collaborative filtering recommendations\n    uu_recommendations, uu_scores = recommend_articles_user_user_with_scores(user_index, df_content, top_n)\n\n    # Combine recommendations and scores\n    combined_scores = {}\n\n    # Iterate through content-based recommendations\n    for idx, score in zip(cb_recommendations.index, cb_scores):\n        body = df_content.loc[idx, 'body_processed']\n        combined_scores[body] = combined_scores.get(body, 0) + score * weight_content_based\n\n    # Iterate through user-user collaborative recommendations\n    for idx, score in zip(uu_recommendations.index, uu_scores):\n        body = df_content.loc[idx, 'body_processed']\n        combined_scores[body] = combined_scores.get(body, 0) + score * weight_user_user\n\n    # Sort articles based on combined scores\n    sorted_articles = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n\n    # Get top N recommendations\n    top_articles = sorted_articles\n\n    return top_articles\n\n# Example usage\nuser_index = 0 \narticle_index = 0\ntop_recommendations = hybrid_recommendations_1(article_index, user_index, telugu_news_df, user_similarity_matrix, user_article_matrix, top_n=10)\n\n# Displaying the hybrid recommendations\nfor article, score in top_recommendations:\n    print(f\"Score: {score:.4f} - Article: {article[:100]}...\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:31:16.223217Z","iopub.execute_input":"2023-12-07T15:31:16.223595Z","iopub.status.idle":"2023-12-07T15:31:32.131741Z","shell.execute_reply.started":"2023-12-07T15:31:16.223561Z","shell.execute_reply":"2023-12-07T15:31:32.130195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hybrid Item-Item Filtering and User-User Filtering","metadata":{}},{"cell_type":"code","source":"def weighted_hybrid_recommendations_item_user(article_index, user_index, df_content, user_similarity_matrix, article_similarity_matrix, top_n=10, weight_item_item=0.7, weight_user_user=0.3):\n    \n    print(f\"User Index: {user_index}\")\n    user_topic_preference = user_preferences[user_index]\n    print(f\"User Topic Preference: {user_topic_preference}\\n\")\n    print(f\"Original Article Index: {article_index}\")\n    print(\"Original Article Details:\")\n    print(\"Title:\", df_content.loc[article_index, 'heading'])\n    print(\"Body:\", df_content.loc[article_index, 'body'][:200] + \"...\\n\")\n    \n    # Item-Item Collaborative Filtering recommendations and scores\n    ii_recommendations, ii_scores = recommend_similar_articles_with_scores(article_index, df_content, top_n)\n\n    # User-User Collaborative Filtering recommendations and scores\n    uu_recommendations, uu_scores = recommend_articles_user_user_with_scores(user_index, df_content, top_n)\n\n    # Combine recommendations and scores with weights\n    combined_scores = {}\n\n    # Processing Item-Item scores\n    for idx, score in zip(ii_recommendations.index, ii_scores):\n        body = df_content.loc[idx, 'body_processed']\n        combined_scores[body] = combined_scores.get(body, 0) + score * weight_item_item\n\n    # Processing User-User scores\n    for idx, score in zip(uu_recommendations.index, uu_scores):\n        body = df_content.loc[idx, 'body_processed']\n        combined_scores[body] = combined_scores.get(body, 0) + score * weight_user_user\n\n    # Sort articles based on combined scores\n    sorted_articles = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n\n    # Get top N recommendations\n    top_articles = [(article, score) for article, score in sorted_articles]\n\n    return top_articles\n\n# Example usage\nuser_index = 0\narticle_index = 0\ntop_recommendations = weighted_hybrid_recommendations_item_user(article_index, user_index, telugu_news_df, user_similarity_matrix, article_similarity_matrix, top_n=10)\n\n# Displaying the hybrid recommendations\nfor article, score in top_recommendations:\n    print(f\"Score: {score:.4f} - Article: {article[:100]}...\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:31:32.133226Z","iopub.execute_input":"2023-12-07T15:31:32.133692Z","iopub.status.idle":"2023-12-07T15:31:32.171178Z","shell.execute_reply.started":"2023-12-07T15:31:32.133656Z","shell.execute_reply":"2023-12-07T15:31:32.169902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hybrid Item-Item filtering and Matrix Factorization ","metadata":{}},{"cell_type":"code","source":"def weighted_hybrid_recommendations(article_index, user_index, df_content, top_n=10, weight_item_item=0.3, weight_matrix_factorization=0.7):\n    \n    print(f\"User Index: {user_index}\")\n    user_topic_preference = user_preferences[user_index]\n    print(f\"User Topic Preference: {user_topic_preference}\\n\")\n    print(f\"Original Article Index: {article_index}\")\n    print(\"Original Article Details:\")\n    print(\"Title:\", df_content.loc[article_index, 'heading'])\n    print(\"Body:\", df_content.loc[article_index, 'body'][:200] + \"...\\n\")\n\n    # Item-Item Collaborative Filtering recommendations and scores\n    ii_recommendations, ii_scores = recommend_similar_articles_with_scores(article_index, df_content, top_n)\n\n    # Matrix Factorization recommendations and scores\n    mf_recommendations, mf_scores = recommend_articles_svd_with_scores(user_index, df_content, top_n)\n\n    # Combine recommendations and scores with weights\n    combined_scores = {}\n\n    # Processing Item-Item scores\n    for idx, score in zip(ii_recommendations.index, ii_scores):\n        body = df_content.loc[idx, 'body_processed']\n        combined_scores[body] = combined_scores.get(body, 0) + score * weight_item_item\n\n    # Processing Matrix Factorization scores\n    for idx, score in zip(mf_recommendations.index, mf_scores):\n        body = df_content.loc[idx, 'body_processed']\n        combined_scores[body] = combined_scores.get(body, 0) + score * weight_matrix_factorization\n\n    # Sort articles based on combined scores\n    sorted_articles = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n\n    # Get top N recommendations\n    top_articles = [(article, score) for article, score in sorted_articles]\n\n    return top_articles\n\n# Example usage\nuser_index = 0 \narticle_index = 0\ntop_recommendations = weighted_hybrid_recommendations(article_index, user_index, telugu_news_df, top_n=10)\n\n# Displaying the hybrid recommendations\nfor article, score in top_recommendations:\n    print(f\"Score: {score:.4f} - Article: {article[:100]}...\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:31:32.176473Z","iopub.execute_input":"2023-12-07T15:31:32.176835Z","iopub.status.idle":"2023-12-07T15:31:32.209204Z","shell.execute_reply.started":"2023-12-07T15:31:32.176807Z","shell.execute_reply":"2023-12-07T15:31:32.207955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}